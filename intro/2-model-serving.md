# Model Serving in Red Hat OpenShift AI

In general, organizations choose to train a model to enhance their applications portfolio by harnessing the insights generated by the model. This allows them to derive inferences, predictions, or educated guesses from known data points related to specific events or topics of interest.

In this course, you will discover how to make the most of Red Hat OpenShift Data Science for deploying machine learning models.

The key question here is: How can we effectively deliver a model to an inference engine or server? This is crucial because when any application within the organization's portfolio sends a request, the inference engine should be able to respond with predictions generated by the meticulously trained model.

For machine learning models to be truly useful, they must be deployed in a production environment where they can process real-time data and address the specific problems they were created to solve.

Deploying a model in a production environment involves importing the previously trained model, which has been saved in a specific file format, into a runtime engine. Once imported, the model is made available for consumption by applications.

"Consuming from a model" refers to the process in which software applications employ a communication method, often REST/HTTP, to send requests to a server. This server, in turn, forwards the request to the model and provides a response. It's essential to note that the server responsible for processing the request and generating a response based on the model must have access to that specific model.

![Model Serving](media/model_serving.png)

#### Prerequisites
- Basic knowledge of machine learning foundations is recommended.

- Basics of creating and consuming REST APIs programmatically

- Basic knowledge of the Python libraries for machine learning and deep learning

- RHOAI >v4.14 installed with a `DataScienceClusters` already defined

- S3 like object data store, ex. [MINIO](https://gitlab.com/rcardona/ocp4-tasks/-/blob/master/storage/minio-object-storage.md?ref_type=heads)

#### Objectives
- Deploy and serve machine learning models on OpenShift

- Model deployment with RHOAI model serving

- Model serving authentication and authorization

- Troubleshoot deployed models

- Send inference requests to deployed models via REST API calls

## Concepts

#### Model Serving

Machine learning model serving encompasses the actions of deploying, overseeing, and making machine learning models accessible for inference, enabling them to provide predictions or classifications for novel, unprocessed data. Following the training and optimization of a machine learning model for a particular task, the serving process entails making the model accessible for both real-time and batch predictions within a production setting. This represents a pivotal stage in the machine learning journey, as it facilitates the seamless integration of models into applications, systems, or services, empowering them to deliver tangible benefits in practical, real-world situations.

#### Model Server Runtime

The terms "model server runtime" and "inference engine" hold a close relationship within the context of deploying and serving machine learning models, yet they denote distinct components within the overarching system.

A model server runtime serves as the execution environment or platform wherein a trained machine learning model operates to generate predictions or inferences. Its responsibilities encompass loading the model into memory, managing client requests, conducting inferences, and furnishing the results. This runtime establishes the essential infrastructure for hosting the model, resource management, and the efficient delivery of model predictions in a production setting.

It's worth noting that the model server runtime can be integrated into a more extensive deployment framework or service, offering features such as scalability, version control, monitoring, and security. Prominent instances of model server runtimes encompass TensorFlow Serving, TorchServe, and ONNX Runtime. These runtimes facilitate the deployment of models crafted using popular machine learning frameworks, offering a standardized means to serve predictions via Application Programming Interfaces (APIs).

#### Inference Engine

An inference engine assumes the role of a component entrusted with executing the forward pass of a machine learning model, thereby generating predictions based on input data. This engine stands as a pivotal element within the model server runtime, purpose-built to efficiently carry out inference tasks. It takes charge of critical optimizations, including hardware acceleration and parallelization, ensuring swift predictions while keeping resource utilization to a minimum.

Depending on the particular architecture, the inference engine can either be integrated into the model server runtime or function alongside it. For instance, TensorFlow Serving seamlessly incorporates TensorFlow's inference engine, while ONNX Runtime serves as both a runtime environment and an inference engine for models adhering to the Open Neural Network Exchange (ONNX) format.

In summary, the model server runtime furnishes the overarching environment for hosting and overseeing machine learning models in a production setting. In contrast, the inference engine bears the responsibility for the actual computation of predictions during inference. These two components collaborate harmoniously to offer a scalable, efficient, and dependable solution for serving machine learning models in real-world applications. The choice of model server runtime and inference engine hinges on factors such as the employed machine learning framework, deployment prerequisites, and the specific optimizations necessitated for the target hardware.

## Model Serving Lab

#### 0 - Configuring Runtime

- 0.0 - Create a data science project called `simple-model-serviing`, then create a `Standard Data Science` workbench. Then, open the workbench to go to the JupyterLab interface.

  ![workbench](media/workbench_options.png)

- 0.1 - Import following repository to the workbench

  - [https://github.com/RedHatQuickCourses/rhods-qc-apps.git](https://github.com/RedHatQuickCourses/rhods-qc-apps.git)

- 0.2 - Open the notebook purchase-amount from the rhods-qc-apps/4.rhods-deploy/chapter1/purchase-amount.ipynb directory

  ![purchase-amount-notebook](media/purchase-amount-notebook.png)

- 0.3 - Run the notebook, and notice the creation of a new file in your environment, the mymodel.pkl

  ![mymodel-pkl](media/mymodel-pkl.png)

> $`\textcolor{red}{\text{ !! IMPORTANT !! }}`$

There are different formats and libraries to export the model. The use of either of those formats depend on the target server runtime, some of them are proven to be more efficient than others for certain type of training algorithms and model sizes. In this case it is using pickle. Other common formats are: __Protobuf, MLeap, H5, ONNX, PMML, Torch.__

- 0.4 Use the model in another notebook

  - 0.4.0 Open the notebook use-purchase-amount from the rhods-qc-apps/4.rhods-deploy/chapter1/use-purchase-amount.ipynb directory

    ![use-purchase-amount-notebook](media/use-purchase-amount-notebook.png)

  - 0.4.1 Run the use-purchase-amount notebook and notice the result

    - You can get the same result without training the model again.

    - You are not training the model in the user-purchase-amount notebook, you are re-using the output from the training notebook, and using the generated model to generate an inference.

> $`\textcolor{lightgreen}{\text{ !! TIP !! }}`$

At this moment the model can be exported and imported in other projects for its use. Normally there will be an S3 bucket or a model registry to store models and versions of such models, and instead of manually exporting the model, there would be pipelines making the model available.


#### 1 - Tranining Model

By default, Red Hat OpenShift AI includes a pre-configured model serving runtime, OpenVINO, which can load, execute, and expose models trained with TensorFlow and PyTorch. OpenVINO supports various model formats, such as the following ones.

  - __ONNX__: An open standard for machine learning interoperability.

  - __OpenVino IR__: The proprietary model format of OpenVINO, the model serving runtime used in OpenShift AI.

In order to leverage the benefits of OpenVINO, you must:

  - Export the model in a format compatible with one of the available RHOAI runtimes.

  - Upload the model to an S3

  - Create a Data Connection to the S3 containing the model

  - Create or use one of the available serving runtimes in a Model Server configuration that specifies the size and resources to use while setting up an inference engine.

  - Start a model server instance to publish your model for consumption

While publishing this model server instance, the configurations will allow you to define how applications securely connect to your model server to request for predictions, and the resources that it can provide.

  - 1.0 Find the MinIO endpoint

    ```bash
    oc get routes -n minio | grep minio-api | awk '{print $2}'
    ```

  - 1.1 Training The Model

    - 1.1.0 Using a JupyterLab workbench at RHOAI, import the repository. It is recommended to use a workbench that was created with the Standard Data Science Notebook image.

      - [https://github.com/RedHatQuickCourses/rhods-qc-apps.git](https://github.com/RedHatQuickCourses/rhods-qc-apps.git)

   - 1.2 Open and run the notebook iris_to_onnx from rhods-qc-apps/4.rhods-deploy/chapter2 directory

     ![iris_training_onnx](media/iris_training_onnx.png)
 
     > $`\textcolor{lightblue}{\text{ !! NOTE !! }}`$
     
     Converting a model to ONNX format depends on the library that you use to create the model. In this case, the model is created with Scikit-Learn, so you must use the sklearn-onnx library to perform the conversion.

     - To convert from PyTorch, see Introduction to ONNX in the PyTorch docs.

     - To convert from TensorFlow, use the tf2onnx library.

   - 1.3 Observe that a file has been created: rf_iris.onnx, download this file to your computer, so that we can upload it to S3.

     ![iris-download](media/iris-download.png)

   - 1.4 Upload the file rf_iris.onnx to a bucket named models, with a path iris in your S3

     ![iris-s3-upload](media/iris-s3-upload.png)

     > $`\textcolor{red}{\text{ !! IMPORTANT !! }}`$
     
     Make sure to create a new path in your bucket, and upload to such path, not to root. Later, when requesting to deploy a model to the Model Server, you will be required to provide a path inside your bucket.

#### 2 - Create A Data Connection

  - 2.0 In the RHOAI dashboard, create a project named iris-project.

  - 2.1 In the __Data Connections__ section, create a Data Connection to your S3

    ![add-minio-iris-data-connection](media/add-minio-iris-data-connection.png)

    > $`\textcolor{red}{\text{ !! IMPORTANT !! }}`$

    - The credentials (Access Key/Secret Key) are minio/minio123.

    - Make sure to use the API route, not the UI route (oc get routes -n object-datastore | grep minio-api | awk '{print $2}').

    - The region is not important when using MinIO, this is a property that has effects when using AWS S3.

    - Mind typos for the bucket name.

    - You don’t have to select a workbench to attach this data connection to.   


#### 3 - Create a Model Server

  - 3.0 In the Models and model servers section, add a server.

  ![add-server-button](media/add-server-button.png)

  - 3.1 Fill the form with the example values

  ![add-server-form-example](media/add-server-form-example.png)
 
  > $`\textcolor{red}{\text{ !! IMPORTANT !! }}`$

  The model server you are creating works as a template for deploying models. As you can see, we have not specified the model that we will deploy, or the data connection from where that model will be retrieved, in this form we are specifying the resources, constraints, and engine that will define the engine where the model will be deployed later.

  - 3.2 After clicking the Add button at the bottom of the form, you will be able to see a new Model Server configuration in your project, you can click the Tokens column, which will make visible the tokens that you can share with the applications that will consume the inference API.

  ![model-server-with-token](media/model-server-with-token.png)

#### 4 - Deploy The Model

  - 4.0 At the right side of the Model Server, we can find the Deploy Model button, let’s click the Deploy Model button, to start filling the Deploy Model form

  ![deploy-model-button](media/deploy-model-button.png)

  - 4.1 Fill the Deploy Model from as in the example

  ![deploy-model-form](media/deploy-model-form.png)

  - 4.2 After clicking the Add button at the bottom of the form, you will be able to see a new entry at the Deployed models column for your Model Server, clicking in the column will eventually show a check mark under the Status column

  ![deploy-model-success](media/deploy-model-success.png)

  - 4.3 Observe and monitor the assets created in your OpenShift iris-project namespace

  ```bash
  oc get routes -n iris-project
  ```

  ```bash
  oc get secrets -n iris-project | grep iris-model
  ```

  ```bash
  oc get events -n iris-project
  ```

  > $`\textcolor{lightgreen}{\text{ !! TIP !! }}`$

  Deploying a Model Server triggers a ReplicaSet with ModelMesh, which attach your model to the inference runtime, and exposes it through a route. Also, notice the creation of a secret with your token.

#### 5 - Test The Model

  - 5.0 Assign the route to an environment variable in your local machine, so that we can use it in our curl commands

    ```bash
    export IRIS_ROUTE=https://$(oc get routes -n iris-project | grep iris-model | awk '{print $2}')
    ```

  - 5.1 Assign an authentication token to an environment variable in your local machine

    ```bash
    export TOKEN=$(oc whoami -t)
    ```

    ```bash
    curl -H "Authorization: Bearer $TOKEN" $IRIS_ROUTE/v2/models/iris-model/infer -X POST --data '{"inputs" : [{"name" : "X","shape" : [ 1, 4 ],"datatype" : "FP32","data" : [ 3, 4, 3, 2 ]}],"outputs" : [{"name" : "output0"}]}'
    ```

